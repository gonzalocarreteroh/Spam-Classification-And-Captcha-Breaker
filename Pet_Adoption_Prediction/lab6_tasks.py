# -*- coding: utf-8 -*-
"""lab6_tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cg8LNHaKRDLxAcDKrG5DqNXPj37RzQVn

# **COMP 2211 Exploring Artificial Intelligence** #
## Lab 6: Multi-Layer Perceptron (MLP) ##

![mlp.jpg](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjH2GbE1DXODQ8tnCVkn8S7F4KiCfaC0RvFt3I2L8d6YQplfHbKWGkaU20EwY7zWj9TjLxn1WdKey8kpqnox_KAKmS3qn0fN2kRLht9tr1bGNkcLgNiK3IUKJiAYd1bP5IXsaKc5s__4H0nLkLw2m2kNLoOU7R_hMpBXfDES-NxIFPPY_RBOpUkoUt-aQ/w613-h345/Multi-layer%20perceptron%20intro%20image.png "Image Source: https://www.pycodemates.com/2023/01/multi-layer-perceptron-a-complete-overview.html")

<font size="1">Image Source: https://www.pycodemates.com/2023/01/multi-layer-perceptron-a-complete-overview.html</font>

## Task 0: Mount Google Drive and Import Libraries
Download and save a copy of the Lab6 Notebook and Datasets (*test_labels.csv*) to your Google Drive, ensuring that all these files are in the same location.

if __name__ == '__main__':
  from google.colab import drive
  drive.mount("/content/drive")

## Dataset Description

We will use a simplified version of the PetFinder [dataset](https://www.kaggle.com/c/petfinder-adoption-prediction).
There are several thousand rows in the CSV. Each row describes a pet, and each column describes an attribute.
We will use this information to predict the speed at which the pet will be adopted.

The following is a description of this dataset. Notice there are both numeric and categorical columns. There is a free text column that we will not use in this tutorial.

Column | Description| Feature Type | Data Type
------------|--------------------|----------------------|-----------------
Type | Type of animal (Dog, Cat) | Categorical | string
Age |  Age of the pet | Numerical | integer
Breed1 | Primary breed of the pet | Categorical | string
Color1 | Color 1 of pet | Categorical | string
Color2 | Color 2 of pet | Categorical | string
MaturitySize | Size at maturity | Categorical | string
FurLength | Fur length | Categorical | string
Vaccinated | Pet has been vaccinated | Categorical | string
Sterilized | Pet has been sterilized | Categorical | string
Health | Health Condition | Categorical | string
Fee | Adoption Fee | Numerical | integer
Description | Profile write-up for this pet | Text | string
PhotoAmt | Total uploaded photos for this pet | Numerical | integer
AdoptionSpeed | Speed of adoption | Classification | integer

Load required libraries.
"""

import numpy as np
import pandas as pd

# ! pip install tensorflow==2.11.0  # comment this line for execution in your local development environment
import tensorflow as tf

from tensorflow import feature_column
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

# print('tensorflow version:',tf.__version__)

"""## Use Pandas to create a dataframe

[Pandas](https://pandas.pydata.org/) is a Python library with many helpful utilities for loading and working with structured data. We will use Pandas to load the dataset into a dataframe.

# Colab
dataframe = pd.read_csv("drive/MyDrive/lab6/petfinder-mini.csv")

# Your local development environment
# dataframe = pd.read_csv("petfinder-mini.csv")

dataframe.head()

Check dataset profile.
"""

# dataframe.info()

"""## Create target variable

The task in the original dataset is to predict the speed at which a pet will be adopted (e.g., in the first week, the first month, the first three months, and so on). Let's simplify this for our tutorial. Here, we will transform this into a binary classification problem, and simply predict whether the pet was adopted or not.

After modifying the label column, 0 will indicate the pet was not adopted, and 1 will indicate it was.

# In the original dataset "4" indicates the pet was not adopted.
dataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)

# Drop un-used columns.
dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])
"""

# dataframe.head(20)

"""## Task 1: Dataset Splitting and Preprocessing

We will randomly shuffle and then split the dataset into the training set `train`, validation set `val`, and testing set `test`. We will wrap the Dataframes with [tf.data](https://www.tensorflow.org/guide/datasets). This will enable us to use feature columns as a bridge to map from the columns in the Pandas Dataframe to features used to train the model.

**Task**: Implement the function `df_to_dataset()`.

Input:
- `dataframe`: A Pandas Dataframe containing the data, including a 'target' column.
- `shuffle`: A boolean indicating whether to shuffle the dataset (default is True).
- `batch_size`: An integer specifying the number of samples per batch (default is 32).

Output:
- A `tf.data.Dataset` object containing the features and labels, optionally shuffled and batched.
"""

"""
train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print('Train examples: ', len(train))
print('Validation examples: ', len(val))
print('Test examples: ',  len(test), )
"""

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
    # TODO: Your code for Task 1 here
    # Use soft copy to maintain the original dataframe (hint: use library function `copy` of dataframe rather than '=' assignment)
    dataframe = dataframe.copy()
    # Remove the 'target' column from the dataframe and store it in 'labels' (hint: use library function `dataframe.pop()`)
    labels = dataframe.pop('target')
    # Create a TensorFlow dataset from the dataframe and labels (hint: use library function `tf.data.Dataset.from_tensor_slices()`, please refer to https://www.tensorflow.org/api_docs/python/tf/data/Dataset)
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    # Shuffle the dataset if 'shuffle' is set to True (hint: the library function `shuffle` is a method of your TensorFlow dataset; please specify the random seed as 20241109 to obtain the same data shuffle result among multiple executions of this codeblock)
    if shuffle:
        ds = ds.shuffle(buffer_size=len(dataframe), seed=20241109)
    # Batch the dataset with the specified batch size (hint: the library function `batch` is a method of your TensorFlow dataset)
    ds = ds.batch(batch_size)
    # Return the prepared TensorFlow dataset
    return ds

"""
batch_size = 5
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)
"""

"""## Understand the input pipeline

Now that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable. Notably, if you did not specify the random seed when calling the dataset shuffle function, you will observe different outputs per run.

for feature_batch, label_batch in train_ds.take(1):
    print('Every feature:', list(feature_batch.keys()))
    print('A batch of ages:', feature_batch['Age'])
    print('A batch of targets:', label_batch)

## Feature engineering (out of scope)
TensorFlow provides many types of feature columns. In this section, we will create several types of feature columns and demonstrate how they transform a column from the dataframe. This part is out of the scope of this lab. You may take a quick look at the content if you would like to.

# We will use this batch to demonstrate several types of feature columns
example_batch = next(iter(train_ds))[0]

# A utility method to create a feature column
# and to transform a batch of data
def demo(feature_column):
    feature_layer = layers.DenseFeatures(feature_column)
    print(feature_layer(example_batch).numpy())


photo_count = feature_column.numeric_column('PhotoAmt')
demo(photo_count)


age = feature_column.numeric_column('Age')
age_buckets = feature_column.bucketized_column(age, boundaries=[1, 3, 5])
demo(age_buckets)


animal_type = feature_column.categorical_column_with_vocabulary_list('Type', ['Cat', 'Dog'])

animal_type_one_hot = feature_column.indicator_column(animal_type)
demo(animal_type_one_hot)


# Notice the input to the embedding column is the categorical column
# we previously created
breed1 = feature_column.categorical_column_with_vocabulary_list('Breed1', dataframe.Breed1.unique())
breed1_embedding = feature_column.embedding_column(breed1, dimension=8)
demo(breed1_embedding)



breed1_hashed = feature_column.categorical_column_with_hash_bucket('Breed1', hash_bucket_size=10)
demo(feature_column.indicator_column(breed1_hashed))


crossed_feature = feature_column.crossed_column([age_buckets, animal_type], hash_bucket_size=10)
demo(feature_column.indicator_column(crossed_feature))

## Choose which columns to use
We have seen how to use several types of feature columns. Now we will use them to train a model. The goal of this tutorial is to show you the complete code (e.g. mechanics) needed to work with feature columns. We have selected a few columns to train our model below, arbitrarily.

Key point: If your aim is to build an accurate model, try a larger dataset of your own, and think carefully about which features are the most meaningful to include, and how they should be represented.

feature_columns = []

# numeric cols
for header in ['PhotoAmt', 'Fee', 'Age']:
    feature_columns.append(feature_column.numeric_column(header))

# bucketized cols
age = feature_column.numeric_column('Age')
age_buckets = feature_column.bucketized_column(age, boundaries=[1, 2, 3, 4, 5])
feature_columns.append(age_buckets)

# indicator_columns
indicator_column_names = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'Sterilized', 'Health']
for col_name in indicator_column_names:
    categorical_column = feature_column.categorical_column_with_vocabulary_list(col_name, dataframe[col_name].unique())
    indicator_column = feature_column.indicator_column(categorical_column)
    feature_columns.append(indicator_column)

# embedding columns
breed1 = feature_column.categorical_column_with_vocabulary_list('Breed1', dataframe.Breed1.unique())
breed1_embedding = feature_column.embedding_column(breed1, dimension=8)
feature_columns.append(breed1_embedding)

# crossed columns
age_type_feature = feature_column.crossed_column([age_buckets, animal_type], hash_bucket_size=100)
feature_columns.append(feature_column.indicator_column(age_type_feature))

## Create a feature layer
Now that we have defined our feature columns, we will use a [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) layer to input them into our Keras model.
"""

# feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

"""Earlier, we used a small batch size to demonstrate how feature columns worked. We will create a new input pipeline with a larger batch size.

# You may customize the batch_size by yourself, as part of hyperparameter tuning
batch_size = 256

train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

## Task 2: MLP Model Building, Compiling, and Training

**Task 2.1**: Implement an MLP model according to the following rules:
- Right after the `feature_layer`, there are two hidden `Dense` layers followed by a `Dropout` operator.
- The activation for hidden `Dense` layer should be `ReLU`.
- The output `Dense` layer should be designed for our `binary classification` task.

The function should return the built model. Please note that you do not need to call `model.compile()` or `model.fit()` in this function because they are follow-up tasks.
"""

# Function for MLP model initialization
def get_model():
    # TODO: Your code for Task 2.1 here
    model = tf.keras.Sequential([
        feature_layer,
        # The first Dense layer
        tf.keras.layers.Dense(128, activation='relu'),
        # The second Dense layer
        tf.keras.layers.Dense(128, activation='relu'),
        # The Dropout operator, you may assign the dropout rate with 0.1
        tf.keras.layers.Dropout(0.1),
        # The output layer
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    # return the initialized model
    return model

# model = get_model()

"""**Task 2.2**: Implement the compile method for an MLP according to the following rules:
- Use `Adam` optimizer with a configurable parameter `learning_rate`.
- The loss function should be one of the `Binary Loss` family.
- Employ `accuracy` as a validation metric.


**Task 2.3**: Implement the training method for an MLP.
"""

# Function for MLP model compiling
def compile_model(model):
    # TODO: Your code for Task 2.2 here
    model.compile(
        # optimizer
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        # loss function
        loss=tf.keras.losses.BinaryCrossentropy(),
        # metrics specification
        metrics=['accuracy']
    )
    # return the compiled model
    return model

# learning_rate = 0.001
# We use the side effect of your function
# model = compile_model(model)


# MLP training
train_history = model.fit(
    # TODO: Your code for Task 2.3 here
    # training data
    train_ds,
    # validation data
    validation_data=val_ds,
    # epoch configuration
    epochs=20
)

"""loss, accuracy = model.evaluate(test_ds)
print("Accuracy", accuracy)

def visualize_training(history, lw = 3):
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10,10))
    plt.subplot(2,1,1)
    plt.plot(history.history['accuracy'], label = 'training', marker = '*', linewidth = lw)
    plt.plot(history.history['val_accuracy'], label = 'validation', marker = 'o', linewidth = lw)
    plt.title('Accuracy Comparison')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.grid(True)
    plt.legend(fontsize = 'x-large')


    plt.subplot(2,1,2)
    plt.plot(history.history['loss'], label = 'training', marker = '*', linewidth = lw)
    plt.plot(history.history['val_loss'], label = 'validation', marker = 'o', linewidth = lw)
    plt.title('Loss Comparison')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend(fontsize = 'x-large')
    plt.grid(True)
    plt.show()

#     plt.figure(figsize=(10,5))
#     plt.plot(history.history['lr'], label = 'lr', marker = '*',linewidth = lw)
#     plt.title('Learning Rate')
#     plt.xlabel('Epochs')
#     plt.ylabel('Learning Rate')
#     plt.grid(True)
#     plt.show()
"""

# visualize_training(train_history)

"""You may save the model by running the following code.

model.save_weights("mlp_model.weights.h5")
model.summary()

You may load the model you just saved and check the accuracy again.

mlp_model = get_model()
mlp_model = compile_model(mlp_model)
mlp_model.evaluate(test_ds)

mlp_model.load_weights("mlp_model.weights.h5")

loss, accuracy = mlp_model.evaluate(test_ds)
print("Accuracy", accuracy)

# Submission for Lab 6

Please export your notebook on Colab as `lab6_tasks.py` (File -> Download -> Download .py), and submit it together with your `mlp_model.weights.h5` model weight file.

![image](assets/export_py.jpeg)
"""